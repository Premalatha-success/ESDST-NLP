{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0cfadd",
   "metadata": {},
   "source": [
    "### Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29178e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the training data\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=1000, verbose=0)\n",
    "\n",
    "# Test the model\n",
    "X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_test = np.array([0, 1, 1, 0])\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b876b",
   "metadata": {},
   "source": [
    "##### In this example, the neural network has one hidden layer with 10 neurons, an input dimension of 2 \n",
    "##### (since we are using 2 input features), and an output layer with one neuron. \n",
    "##### The activation function used for the hidden layer is ReLU, and the output layer uses a sigmoid activation function. \n",
    "##### The loss function used is binary cross-entropy, and the optimizer used is Adam.\n",
    "##### The training data consists of four input-output pairs, which represent the XOR function.\n",
    "##### The model is trained for 1000 epochs. Finally, the accuracy of the model is evaluated using the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d90335e",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44ce005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=32, input_shape=(None, 1), activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the training data\n",
    "X_train = np.array([[[0], [1], [0], [1], [0]],\n",
    "                    [[1], [0], [1], [0], [1]],\n",
    "                    [[0], [1], [1], [1], [0]],\n",
    "                    [[1], [1], [1], [1], [1]]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "\n",
    "# Test the model\n",
    "X_test = np.array([[[1], [0], [1], [0], [1]],\n",
    "                   [[0], [1], [0], [1], [0]],\n",
    "                   [[1], [1], [0], [1], [0]],\n",
    "                   [[0], [1], [1], [1], [1]]])\n",
    "y_test = np.array([0, 0, 1, 1])\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f389db",
   "metadata": {},
   "source": [
    "##### In this example, we are implementing a simple RNN with one layer of 32 units, using the SimpleRNN layer from Keras. \n",
    "##### The RNN takes a sequence of input data, with each time step containing one feature. \n",
    "##### The activation function used is ReLU for the RNN layer and sigmoid for the output layer. \n",
    "##### The loss function used is binary cross-entropy, and the optimizer used is Adam.\n",
    "##### The training data consists of four input-output pairs, which represent a binary sequence pattern. \n",
    "##### The model is trained for 100 epochs. Finally, the accuracy of the model is evaluated using the test data.\n",
    "##### Note that in this example, the input data is of shape (batch_size, timesteps, input_dim). \n",
    "##### Since we have a one-dimensional sequence of input data with five time steps, the input shape is (4, 5, 1). \n",
    "##### The None in the input shape means that the length of the input sequence can vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8bed90",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b0072f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=32, input_shape=(1, 5), activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the training data\n",
    "X_train = np.array([[[1, 0, 1, 0, 1]],\n",
    "                    [[0, 1, 0, 1, 0]],\n",
    "                    [[1, 1, 0, 1, 0]],\n",
    "                    [[0, 1, 1, 1, 0]]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "\n",
    "# Test the model\n",
    "X_test = np.array([[[0, 1, 0, 1, 0]],\n",
    "                   [[1, 0, 1, 0, 1]],\n",
    "                   [[1, 1, 1, 1, 0]],\n",
    "                   [[0, 1, 1, 0, 1]]])\n",
    "y_test = np.array([0, 0, 1, 1])\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1267a0",
   "metadata": {},
   "source": [
    "##### In this example, we are implementing a simple LSTM with one layer of 32 units, using the LSTM layer from Keras. \n",
    "##### The LSTM takes a sequence of input data, with each time step containing a five-dimensional feature vector. \n",
    "##### The activation function used is ReLU for the LSTM layer and sigmoid for the output layer.\n",
    "##### The loss function used is binary cross-entropy, and the optimizer used is Adam.\n",
    "\n",
    "##### The training data consists of four input-output pairs, which represent a binary sequence pattern. \n",
    "##### The model is trained for 100 epochs. Finally, the accuracy of the model is evaluated using the test data.\n",
    "\n",
    "##### Note that in this example, the input data is of shape (batch_size, timesteps, input_dim). \n",
    "##### Since we have a one-dimensional sequence of input data with one time step and five features, the input shape is (4, 1, 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739aad9",
   "metadata": {},
   "source": [
    "### GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7f3eb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00\n"
     ]
    }
   ],
   "source": [
    "### Importing the required libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense\n",
    "\n",
    "# Define the GRU model\n",
    "model = Sequential()\n",
    "model.add(GRU(units=32, input_shape=(1, 5), activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the training data\n",
    "X_train = np.array([[[1, 0, 1, 0, 1]],\n",
    "                    [[0, 1, 0, 1, 0]],\n",
    "                    [[1, 1, 0, 1, 0]],\n",
    "                    [[0, 1, 1, 1, 0]]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "\n",
    "# Test the model\n",
    "X_test = np.array([[[0, 1, 0, 1, 0]],\n",
    "                   [[1, 0, 1, 0, 1]],\n",
    "                   [[1, 1, 1, 1, 0]],\n",
    "                   [[0, 1, 1, 0, 1]]])\n",
    "y_test = np.array([0, 0, 1, 1])\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304693c1",
   "metadata": {},
   "source": [
    "##### In this example, we are implementing a simple GRU with one layer of 32 units, using the GRU layer from Keras. \n",
    "##### The GRU takes a sequence of input data, with each time step containing a five-dimensional feature vector. \n",
    "##### The activation function used is ReLU for the GRU layer and sigmoid for the output layer. \n",
    "##### The loss function used is binary cross-entropy, and the optimizer used is Adam.\n",
    "##### The training data consists of four input-output pairs, which represent a binary sequence pattern. \n",
    "##### The model is trained for 100 epochs. Finally, the accuracy of the model is evaluated using the test data.\n",
    "##### Note that in this example, the input data is of shape (batch_size, timesteps, input_dim). \n",
    "##### Since we have a one-dimensional sequence of input data with one time step and five features, the input shape is (4, 1, 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bb73e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.6959 - accuracy: 0.7757WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x000001F5BB2E84C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "469/469 [==============================] - 45s 76ms/step - loss: 0.6959 - accuracy: 0.7757 - val_loss: 0.2779 - val_accuracy: 0.9167\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.2074 - accuracy: 0.9387 - val_loss: 0.1613 - val_accuracy: 0.9524\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 31s 65ms/step - loss: 0.1429 - accuracy: 0.9580 - val_loss: 0.1162 - val_accuracy: 0.9660\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 31s 67ms/step - loss: 0.1140 - accuracy: 0.9660 - val_loss: 0.1023 - val_accuracy: 0.9704\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 32s 69ms/step - loss: 0.0947 - accuracy: 0.9716 - val_loss: 0.0836 - val_accuracy: 0.9764\n",
      "Epoch 1/5\n",
      "469/469 [==============================] - 40s 66ms/step - loss: 0.8361 - accuracy: 0.7204 - val_loss: 0.3022 - val_accuracy: 0.9141\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 32s 67ms/step - loss: 0.2423 - accuracy: 0.9297 - val_loss: 0.1764 - val_accuracy: 0.9498\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.1634 - accuracy: 0.9517 - val_loss: 0.1326 - val_accuracy: 0.9628\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 0.1259 - accuracy: 0.9639 - val_loss: 0.1153 - val_accuracy: 0.9658\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 0.1066 - accuracy: 0.9690 - val_loss: 0.0972 - val_accuracy: 0.9718\n",
      "Epoch 1/5\n",
      "469/469 [==============================] - 15s 24ms/step - loss: 0.7132 - accuracy: 0.7776 - val_loss: 0.3534 - val_accuracy: 0.8989\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 0.3095 - accuracy: 0.9091 - val_loss: 0.2553 - val_accuracy: 0.9265\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 0.2392 - accuracy: 0.9315 - val_loss: 0.2058 - val_accuracy: 0.9417\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 0.2034 - accuracy: 0.9416 - val_loss: 0.1819 - val_accuracy: 0.9470\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 0.1780 - accuracy: 0.9488 - val_loss: 0.1872 - val_accuracy: 0.9446\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.0836 - accuracy: 0.9764\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0972 - accuracy: 0.9718\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1872 - accuracy: 0.9446\n",
      "LSTM accuracy: 0.9764000177383423\n",
      "GRU accuracy: 0.9718000292778015\n",
      "RNN accuracy: 0.944599986076355\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, SimpleRNN, Dense\n",
    "\n",
    "# Load the dataset\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "y_train = np.eye(10)[y_train.astype('int32')]\n",
    "y_test = np.eye(10)[y_test.astype('int32')]\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(units=64, input_shape=(28, 28), activation='tanh'))\n",
    "lstm_model.add(Dense(units=10, activation='softmax'))\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the GRU model\n",
    "gru_model = Sequential()\n",
    "gru_model.add(GRU(units=64, input_shape=(28, 28), activation='tanh'))\n",
    "gru_model.add(Dense(units=10, activation='softmax'))\n",
    "gru_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the RNN model\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(SimpleRNN(units=64, input_shape=(28, 28), activation='tanh'))\n",
    "rnn_model.add(Dense(units=10, activation='softmax'))\n",
    "rnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the models\n",
    "lstm_model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))\n",
    "gru_model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))\n",
    "rnn_model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the models\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "gru_loss, gru_accuracy = gru_model.evaluate(X_test, y_test)\n",
    "rnn_loss, rnn_accuracy = rnn_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print('LSTM accuracy:', lstm_accuracy)\n",
    "print('GRU accuracy:', gru_accuracy)\n",
    "print('RNN accuracy:', rnn_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29bd2d",
   "metadata": {},
   "source": [
    "##### In this example, we are using the MNIST dataset, which consists of 60,000 training images \n",
    "#####  and 10,000 testing images of handwritten digits. \n",
    "##### We preprocess the data by normalizing the pixel values and converting the labels into one-hot encoded vectors.\n",
    "\n",
    "##### We define three models: an LSTM model, a GRU model, and a simple RNN model. Each model takes an input \n",
    "##### sequence of 28x28 pixel images, and outputs a probability distribution over the 10 possible classes (digits 0-9). \n",
    "##### The models are trained for 5 epochs with a batch size of 128. The loss function used is categorical cross-entropy, \n",
    "##### and the optimizer used is Adam.\n",
    "\n",
    "##### After training, we evaluate the models on the test set and print the accuracy for each model.\n",
    "\n",
    "##### Note that in this example, the input data is of shape (batch_size, timesteps, input_dim). \n",
    "##### Since the MNIST images are 28x28 pixels, the input shape is (batch_size, 28, 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9120713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "196/196 [==============================] - 1068s 5s/step - loss: 0.4498 - accuracy: 0.7768 - val_loss: 0.3102 - val_accuracy: 0.8683\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 1088s 6s/step - loss: 0.2497 - accuracy: 0.9043 - val_loss: 0.3136 - val_accuracy: 0.8714\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 1638s 8s/step - loss: 0.1898 - accuracy: 0.9298 - val_loss: 0.3244 - val_accuracy: 0.8664\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 1135s 6s/step - loss: 0.1398 - accuracy: 0.9498 - val_loss: 0.4076 - val_accuracy: 0.8580\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 1118s 6s/step - loss: 0.1065 - accuracy: 0.9618 - val_loss: 0.4168 - val_accuracy: 0.8576\n",
      "Epoch 1/5\n",
      "196/196 [==============================] - 794s 4s/step - loss: 0.5765 - accuracy: 0.6898 - val_loss: 0.3855 - val_accuracy: 0.8304\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 754s 4s/step - loss: 0.2967 - accuracy: 0.8780 - val_loss: 0.3181 - val_accuracy: 0.8698\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 753s 4s/step - loss: 0.2036 - accuracy: 0.9224 - val_loss: 0.3373 - val_accuracy: 0.8694\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 747s 4s/step - loss: 0.1557 - accuracy: 0.9424 - val_loss: 0.3898 - val_accuracy: 0.8633\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 758s 4s/step - loss: 0.1135 - accuracy: 0.9606 - val_loss: 0.4046 - val_accuracy: 0.8608\n",
      "Epoch 1/5\n",
      "196/196 [==============================] - 195s 958ms/step - loss: 0.6987 - accuracy: 0.5200 - val_loss: 0.6820 - val_accuracy: 0.5366\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 180s 920ms/step - loss: 0.6659 - accuracy: 0.5844 - val_loss: 0.6699 - val_accuracy: 0.5587\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 184s 940ms/step - loss: 0.6532 - accuracy: 0.6066 - val_loss: 0.6494 - val_accuracy: 0.6149\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 184s 938ms/step - loss: 0.6227 - accuracy: 0.6463 - val_loss: 0.6275 - val_accuracy: 0.6368\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 197s 1s/step - loss: 0.5968 - accuracy: 0.6748 - val_loss: 0.6077 - val_accuracy: 0.6546\n",
      "782/782 [==============================] - 135s 173ms/step - loss: 0.4168 - accuracy: 0.8576\n",
      "782/782 [==============================] - 118s 151ms/step - loss: 0.4046 - accuracy: 0.8608\n",
      "782/782 [==============================] - 45s 58ms/step - loss: 0.6077 - accuracy: 0.6546\n",
      "LSTM accuracy: 0.8576400279998779\n",
      "GRU accuracy: 0.860759973526001\n",
      "RNN accuracy: 0.6546000242233276\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the dataset\n",
    "max_features = 10000\n",
    "max_len = 200\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(max_features, 128))\n",
    "lstm_model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the GRU model\n",
    "gru_model = Sequential()\n",
    "gru_model.add(Embedding(max_features, 128))\n",
    "gru_model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "gru_model.add(Dense(1, activation='sigmoid'))\n",
    "gru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the RNN model\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(max_features, 128))\n",
    "rnn_model.add(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the models\n",
    "lstm_model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))\n",
    "gru_model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))\n",
    "rnn_model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the models\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "gru_loss, gru_accuracy = gru_model.evaluate(X_test, y_test)\n",
    "rnn_loss, rnn_accuracy = rnn_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print('LSTM accuracy:', lstm_accuracy)\n",
    "print('GRU accuracy:', gru_accuracy)\n",
    "print('RNN accuracy:', rnn_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90e2ca",
   "metadata": {},
   "source": [
    "##### In this example, we are using the IMDB dataset, which consists of movie reviews labeled as positive or negative. \n",
    "##### We preprocess the data by truncating or padding the reviews to a fixed length of 200 words and using one-hot encoding \n",
    "##### for the labels.\n",
    "\n",
    "##### We define three models: an LSTM model, a GRU model, and a simple RNN model. \n",
    "##### Each model takes an input sequence of word indices, and outputs a single probability representing the \n",
    "##### sentiment (positive or negative) of the review. The models are trained for 5 epochs with a batch size of 128. \n",
    "##### The loss function used is binary cross-entropy, and the optimizer used is Adam.\n",
    "\n",
    "##### After training, we evaluate the models on the test set and print the accuracy for each model. \n",
    "##### Note that since this is a binary classification problem, we are only interested in the accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47919887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
